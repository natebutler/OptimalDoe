---
title: "Introduction to optimalDoe"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Introduction to optimalDoe}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  fig.width = 7,
  fig.height = 4,
  collapse = TRUE,
  comment = "#>"
)
```

An optimal experimental design is one that optimizes, maximizes or minimizes, a function of the information. There a different functions one can optimize and when these functions are optimized, your experimental design will then have desirable traits. This package generates A-optimal and D-optimal designs. When these criteria are optimized, the resulting regression coefficients' standard errors are minimized. 

Before we define the objective functions we used, we first need to introduce some notation. $\mathbf{X}\in\mathbb{R}^{n\times k}$ is our design matrix where $n$ is the number of experimental runs and $k$ is the number of experimental factors being studied. $\mathbf{F}(\mathbf{X})=\mathbf{F}\in\mathbb{R}^{n\times p}$ is the model matrix expanded from the design matrix and $p$ is the number of parameters in the model of interest. 

We define the D-criteria as $D(\mathbf{X})=\frac{1}{\mathbf{F'F}}$ 

We define the A_criteria as $A(\mathbf{X})=tr[(\mathbf{F'F})^{-1}]$ 

This vignette is intended for practitioners hoping to generate an optimal experimental design. This package implements the coordinate exchange algorithm to generate D and A-optimal experimental designs. In addition to generating optimal experimental designs, this package also allows one to score designs on the D-criteria and A-criteria. The package also allows the user to plot fraction of design space signatures to compare prediction variance qualities over the design space. 

This vignette is housed at https://github.com/natebutler/OptimalDoe, where you can install the package using the code below.

## Package Installation
```{r setup, comment = NA}
# devtools::install_github("natebutler/OptimalDoe")
library(optimalDoe)
```
## Generating Designs
Here is how you generate some optimal designs:
```{r, comment = NA, cache = TRUE}
set.seed(2187)
designs_d <- generate_designs(
  n = 10, k = 2, order = 1, criteria = "D",
  iterations = 20
)

designs_a <- generate_designs(
  n = 10, k = 2, order = 1, criteria = "A",
  iterations = 20
)
```
Above, we specify: 

* n - the number of experimental runs 
* k - the number of experimental factors  
* order - order of the model you want 
  + Enter 0 for a first order main effects model 
  + 1 for a first order model with 2 way interactions 
  + 2 for a second order model with 2 way interactions and squared main effects 
* criteria - the scoring criteria used to find the optimal design. Enter 
either 'A' or 'D' to score on A-criteria or D-criteria 
* iterations - the number of iterations you want the coordinate exchange algorithm to run 

The generate_designs function takes in the above parameters and runs the coordinate exchange algorithm to generate either A or D-optimal experimental designs based on the user-specified criteria. It will generate designs with N runs and k factors with the intention of fitting the order of the model specified. The user will then specify the number of iterations and the coordinate exchange algorithm will run that number of times. It should be noted that Brad Jones and Peter Goos recommend running the coordinate exchange algorithm for at least 1000 iterations in their textbook "Optimal Design of Experiments: A Case Study Approach." 

```{r, comment = NA}
head(designs_d)

head(designs_a)
```
The generate_designs function returns an ordered list (by ascending optimality score) where the length is that of the iterations specified. Each element of that list is a list of length 2 where the first element is a locally optimal design matrix and the second is the associated optimality score associated with that matrix. 

It should be noted that these optimality scores have no particular interpretation, but a lower score is better. You would to compare designs that aim to fit the same model, have the same number of experimental runs, and same number of experimental trials. 


## Scoring Functions
While our generate_design function does score generated designs on the specified criteria, users may want to cross-compare scorings of the other criteria. They can do so with the following functions:
```{r, comment = NA}
# Pull out first design matrix of the list.
result_a <- designs_a[[1]][[1]]
result_d <- designs_d[[1]][[1]]

d_crit(design = result_a, order = 1)
a_crit(design = result_a, order = 1)
```
The d_crit function takes a design matrix and a specified order of model (same order as before) and expands the design matrix into the model matrix then calculates and returns the determinant of the inverse of the information matrix. 

The a_crit function takes a design matrix and a specified order of model (same order as before) and expands the design matrix into the model matrix then calculates and returns the trace of the inverse of the information matrix. 

You may want to score your generated D-optimal designs on the A-criteria and A-optimal designs on the D-criteria. The reason for this is that sometimes a generated D-optimal design may also be A-optimal but the generated A-optimal design may not be D-optimal, or the other way around. When that happens, you would most likely want to go with the design that optimizes both objective functions. 

In the code above, we are scoring a generated A-optimal design on both the A-criteria and D-criteria to see how it does on both of the criteria. 

### Singularity Check
Another function, singular_check, is called under the hood of the criteria functions. It's use is to check that the information matrix is not numerically singular. We need to have this check since the objective functions are defined from the inverse of the information matrix. The threshold is machine epsilon, and if this threshold is triggered, a design is then given an undesirable score. 
```{r, comment = NA}
singular_check(x = result_a, order = 1)
```
The function returns TRUE if the information matrix calculated from the given design matrix is singular, and FALSE otherwise.

## Fraction of Design Space Plots
After generating designs, a practitioner may want to look at prediction variance qualities to choose a design with low relative prediction variance over the design space. The fraction of design space plot plots the relative prediction variance over the design space. 

We have generated 100 A and 100 D-optimal designs, each fitting the main effects model with all two-way interactions with 14 experimental runs and 3 experimental factors. Here is how you plot relative prediction variance of the candidate designs we've created for you.
```{r, comment = NA}
set.seed(2199)

# A list of 100 candidate D-optimal designs
optimal_d <- optimalDoe::optimal_D_25_4

# A list of 100 candidate A-optimal designs
optimal_a <- optimalDoe::optimal_A_25_4

# Pulling out the top design from each list
result_a_25_4 <- lapply(head(optimal_a, 1), `[[`, 1)
result_d_25_4 <- lapply(head(optimal_d, 1), `[[`, 1)

# Combining the top 2 designs from each list into one list
matrix_list <- c(result_a_25_4, result_d_25_4)

# Plot relative prediction variances of the designs
fds_plot(des_list = matrix_list, order = 2)
```


The fds_plot function returns lines corresponding to each design matrix. The labeling of the lines comes from the order of the matrices in the list, i.e. the first element is "Design1" and so forth. 

One desirable trait of an experimental design is that it has lower prediction variance over the design space. If you see one of these design signatures falling below another, the design with the lower signature has more desirable prediction variance qualities. Looking at this plot, if the practitioner was interested in making predictions, they may want to consider using design 1, the A-optimal design, instead of design 2, the D-optimal design. 

## Conclusions 

To reiterate, this package gives a user the tools to generate D and A-optimal designs, score the generated designs on the other criteria, and plot relative prediction variance over the design space of generated designs. A statistician hoping to learn more about experimental design or a researcher with at least some statistical background may find this package to be especially helpful.


